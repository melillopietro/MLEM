# MLEM – Machine Learning for Exploit Mapping

MLEM is a Machine Learning project focused on classifying ransomware gangs based on behavioral and technical features. The workflow is structured in modular scripts that handle data cleaning, formatting, stratification, validation, and model training with hyperparameter tuning.

---
### Repository Structure

```
MLEM/
├── best_models/                      # Exported, trained models with best hyperparameters (.pkl)
├── dataset_split/                   # Stratified training/validation/test sets
├── gang_distribution_report.py      # Script to generate the gang distribution graph
├── generate_dataset.py              # Script to generate the different sets (default is 80 - 15 - 15%)
├── dataset_ML_Formatter.py          # One-Hot encoding and formatting
├── final_ml_dataset_encoded.csv     # Clean, encoded dataset for modeling
├── label_mapping.csv                # Mapping of gang names to numeric labels (needed for training)
├── main.py                          # Optional main script
├── model_comparison_results.csv     # Evaluation metrics from all trained models
├── multimodel_gridsearch_pipeline.py # Training + Grid Search logic
├── normalized_dataset.py            # Dataset cleaning and filtering
├── README.md                        # Project documentation
├── requirements.txt                 # Python dependency list
├── stratification_dataset.py        # Dataset stratification logic
├── validate_dataset.py              # Dataset validation and sanity checks
```
---
### Prerequisites

Make sure you have Dataset Ransomware.xlsx in the main directory of the project

---

### Installation

1. Clone the repository or download the project files.

2. (Optional but recommended) Create and activate a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements
```

---
### Workflow

Run the following scripts in the specified order to process the dataset and train the models.

### 1. `normalized_dataset.py`

Normalizes the dataset by mapping sectors to standard categories and normalizing country names.
No feature normalization is needed since all features are binary.

```bash
python3 normalized_dataset.py
```

### 2. `dataset_ML_Formatter.py`

Applies One-Hot Encoding to categorical features, making the dataset compatible with ML models.

```bash
python3 dataset_ML_Formatter.py
```

### 3. `generate_dataset.py`

Splits the dataset into training, validation, and test sets, preserving class distribution.

```bash
python3 generate_dataset.py
```

### 4. `startification_dataset.py`

Evaluates the stratification of che training - validation - test sets;

```bash
python3 stratification_dataset.py
```

### 5. `multimodel_gridsearch_pipeline.py`

Trains multiple models with Grid Search for hyperparameter tuning.  
Outputs model metrics and saves the best trained models as `.pkl` files.

```bash
python3 multimodel_gridsearch_pipeline.py
```

---

## Results

## model_comparision_results.csv : 
    it shows the best models based on f1 macro and accuracy metrics.

---

## Expected Data Directory

Ensure the following files are present in `./dataset_split` before running the training pipeline:

- `X_train.csv`
- `X_val.csv`
- `X_test.csv`
- `y_train.csv`
- `y_val.csv`
- `y_test.csv`

These are generated by the previous scripts.

---